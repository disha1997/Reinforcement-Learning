{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hTS6f3vv9-ucbOI_OKCgpZmHi5wBlhYG","timestamp":1712992088446},{"file_id":"1vfobY3UFwFRXy0rYXNdqaPniVd-78dtq","timestamp":1712982003845}],"authorship_tag":"ABX9TyPrItc/eE7IE4d9HbfY1DWM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I1TlM4xWBhKT"},"outputs":[],"source":["import itertools\n","import numpy as np\n","import pandas as pd\n","import random"]},{"cell_type":"markdown","source":["Actions encoded as:\n","\n","(0,1) -> right\n","\n","(0,-1) -> left\n","\n","(1,0) -> down\n","\n","(-1,0) -> up"],"metadata":{"id":"eO9lwqp_T7Mo"}},{"cell_type":"code","source":["TERMINAL_STATES = [(3,3)]\n","START_STATE = (0,0)\n","\n","class GridWorldEnv:\n","  def __init__(self, N=10, M=10):\n","    self.N = N\n","    self.M = M\n","    self.observation_space = list(itertools.product(range(N), range(M)))\n","    self.action_space = [(0,1), (0,-1), (1,0), (-1,0)]\n","\n","    self.terminal_states = TERMINAL_STATES\n","    self.reset()\n","\n","  def reset(self):\n","    self.state = START_STATE\n","    self.terminated = False\n","    self.total_reward = 0\n","\n","  def _get_transition_probability(self, start_state, action, end_state):\n","    if start_state in self.terminal_states:\n","      return 0\n","\n","    expected_state = tuple(np.array(start_state) + np.array(action))\n","    if expected_state == end_state:\n","      return 1\n","\n","    if expected_state not in self.observation_space and start_state == end_state:\n","      return 1\n","\n","    return 0\n","\n","  def _get_reward(self, start_state, action, end_state):\n","    if end_state in self.terminal_states:\n","      return 10\n","    else:\n","      return -1\n","\n","  def step(self, action):\n","    \"\"\"Takes the action that agent took as the param and returns the next\n","    state, one-step reward and other necessary attributes.\n","    \"\"\"\n","    # check if terminal state is reached\n","    if self.state in self.terminal_states:\n","      self.terminated = True\n","      reward = np.nan\n","      return self.state, reward, self.terminated\n","\n","    max_prob = 0\n","    current_state = self.state\n","\n","    for possible_state in self.observation_space:\n","      p = self._get_transition_probability(current_state, action, possible_state)\n","      if p > max_prob:\n","        next_state = possible_state\n","        max_prob = p\n","\n","    reward = self._get_reward(current_state, action, next_state)\n","\n","    self.state = next_state\n","    self.total_reward += reward\n","\n","    return self.state, reward, self.terminated"],"metadata":{"id":"TS7H4_sCCNwv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Random steps in Gridworld"],"metadata":{"id":"RTYslM0ZF3Tn"}},{"cell_type":"code","source":["gw = GridWorldEnv(5,5)\n","\n","gw.state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLCciQ6EF5aL","executionInfo":{"status":"ok","timestamp":1712992797923,"user_tz":-330,"elapsed":409,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"ace4d50f-596f-45ac-8c46-48bedabb21fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(\"current state: \", gw.state)\n","current_action = (0,1) # move right\n","\n","next_state, reward, is_terminated = gw.step(current_action)\n","\n","print(\"action: \", current_action)\n","print(\"Next state: \", next_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPG9y2ilGHnZ","executionInfo":{"status":"ok","timestamp":1712992835584,"user_tz":-330,"elapsed":20,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"4e1b95d3-441b-4bbb-8c25-820e97119d08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["current state:  (0, 1)\n","action:  (0, 1)\n","Next state:  (0, 2)\n"]}]},{"cell_type":"code","source":["gw.reset()\n","is_terminated = gw.terminated\n","\n","# i = 0\n","while not is_terminated:\n","  current_state = gw.state\n","  action = random.choices(gw.action_space)[0]\n","  next_state, reward, is_terminated = gw.step(action)\n","  # print(current_state, action, next_state, reward)\n","  # i+=1\n","\n","print(\"total reward: \", gw.total_reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqW8uWFlGv_Q","executionInfo":{"status":"ok","timestamp":1712992884284,"user_tz":-330,"elapsed":435,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"4c21c64d-965e-4b47-b5e7-9b8951c34580"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total reward:  -89\n"]}]},{"cell_type":"code","source":["class RandomActionAgent:\n","  def __init__(self, env):\n","    self.env = env\n","\n","  def policy(self):\n","    action = random.choices(self.env.action_space)[0]\n","    return action"],"metadata":{"id":"XnfcVs-FG6PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent = RandomActionAgent(gw)\n","\n","gw.reset()\n","is_terminated = gw.terminated\n","\n","while not is_terminated:\n","  current_state = gw.state\n","  action = agent.policy()\n","  next_state, reward, is_terminated = gw.step(action)\n","  # print(current_state, action, next_state, reward)\n","\n","print(\"total reward: \", gw.total_reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_ZBCetmK1yS","executionInfo":{"status":"ok","timestamp":1712992978957,"user_tz":-330,"elapsed":599,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"1987a5b0-9c84-4fb6-ac2c-e29e10f12ed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total reward:  -50\n"]}]},{"cell_type":"markdown","source":["### Monte Carlo Prediction - Policy Evaluation"],"metadata":{"id":"WFGg0GXLrXkg"}},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"dRDZ8ywKy5Ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gw = GridWorldEnv(5,5)\n","random_act_agent = RandomActionAgent(gw)\n","\n","V = dict(zip(gw.observation_space, np.zeros(gw.N*gw.M)))\n","# G = V.copy()\n","\n","# history = []\n","gw.reset()\n","\n","for iter in tqdm(range(10000)):\n","  history = []\n","  gw.reset()\n","  # start from a random state to start the episode\n","  gw.state = random.choices(gw.observation_space)[0]\n","\n","  # Simulate one episode\n","  while not gw.terminated:\n","    current_state = gw.state\n","    state, reward , _ = gw.step(random_act_agent.policy())\n","    history.append((current_state,reward))\n","\n","  # update the state values according to the returns received in the\n","  # current episode\n","  G = 0\n","  for i in range(len(history)-2, -1, -1):\n","    G = history[i][1] + 1 * G\n","    V[history[i][0]] = V[history[i][0]] + 0.01 * (G - V[history[i][0]])\n","\n","np.array(list(V.values())).reshape(5,5).round()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc_yAtWcrxkA","executionInfo":{"status":"ok","timestamp":1712994320508,"user_tz":-330,"elapsed":50463,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"b6819fd0-8137-46ec-ed31-3bfd0881ffba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:50<00:00, 198.07it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[-38., -39., -39., -37., -37.],\n","       [-42., -42., -35., -29., -32.],\n","       [-39., -35., -23., -14., -27.],\n","       [-34., -34., -18.,   0., -18.],\n","       [-34., -33., -17., -10., -28.]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### Monte Carlo Control - Policy improvement"],"metadata":{"id":"VHWI4I4ctsJV"}},{"cell_type":"code","source":["# Initialize the environment\n","gw = GridWorldEnv(4, 4)\n","\n","# Initialize state-action values Q(s, a)\n","Q = dict()\n","for state in gw.observation_space:\n","    for action in gw.action_space:\n","        Q[(state, action)] = np.random.random()\n","\n","print(Q)\n","\n","# Number of episodes\n","num_episodes = 10000\n","epsilon = 0.1  # Epsilon-greedy exploration\n","\n","# Monte Carlo On-Policy Prediction\n","for episode in tqdm(range(num_episodes)):\n","    gw.reset()\n","    episode_history = []\n","\n","    # start from a random state to start the episode\n","    gw.state = random.choice(gw.observation_space)\n","\n","    while not gw.terminated:\n","        current_state = gw.state\n","\n","        # Epsilon-greedy policy\n","        if random.random() < epsilon:\n","            action = random.choice(gw.action_space)\n","        else:\n","            action = max(gw.action_space, key=lambda a: Q[(current_state, a)])\n","\n","        next_state, reward, _ = gw.step(action)\n","        episode_history.append((current_state, action, reward))\n","\n","    # Update state-action values based on the returns received in the episode\n","    G = 0\n","    for t in range(len(episode_history) - 2, -1, -1):\n","        state_t, action_t, reward = episode_history[t]\n","        G = reward + 1 * G\n","        # N[(state_t, action_t)] += 1\n","        Q[(state_t, action_t)] += 0.01*(G - Q[(state_t, action_t)])# / N[(state_t, action_t)]\n","\n","    # if episode % 100 == 0:\n","    #   print(Q)\n","\n","Q\n","# # Display the optimal state-action values\n","# optimal_Q_values = np.array([Q[(state, action)] for state in gw.observation_space for action in gw.action_space])\n","# optimal_Q_values = optimal_Q_values.reshape((gw.N, gw.M, len(gw.action_space))).round(1)\n","# print(optimal_Q_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIBp_2Sg5R0X","executionInfo":{"status":"ok","timestamp":1712994768668,"user_tz":-330,"elapsed":3954,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"de84d308-ab4f-4bbc-9199-9b9112a6ed03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{((0, 0), (0, 1)): 0.32065784611432013, ((0, 0), (0, -1)): 0.006855008274554986, ((0, 0), (1, 0)): 0.5842309815769173, ((0, 0), (-1, 0)): 0.21585278235800054, ((0, 1), (0, 1)): 0.9569594138334169, ((0, 1), (0, -1)): 0.6461938302008559, ((0, 1), (1, 0)): 0.11208893759184091, ((0, 1), (-1, 0)): 0.6238019361300273, ((0, 2), (0, 1)): 0.5915255875274025, ((0, 2), (0, -1)): 0.9137597827662408, ((0, 2), (1, 0)): 0.28752730579175567, ((0, 2), (-1, 0)): 0.7269461522173922, ((0, 3), (0, 1)): 0.7079917546430635, ((0, 3), (0, -1)): 0.7135315647078353, ((0, 3), (1, 0)): 0.6525679313801847, ((0, 3), (-1, 0)): 0.4217868400422119, ((1, 0), (0, 1)): 0.20052148897076227, ((1, 0), (0, -1)): 0.7505430984981386, ((1, 0), (1, 0)): 0.036579153423505484, ((1, 0), (-1, 0)): 0.589615502755193, ((1, 1), (0, 1)): 0.7421528323746497, ((1, 1), (0, -1)): 0.5781145364031902, ((1, 1), (1, 0)): 0.08370644773613267, ((1, 1), (-1, 0)): 0.593669156257704, ((1, 2), (0, 1)): 0.5023458129160917, ((1, 2), (0, -1)): 0.3240638819820064, ((1, 2), (1, 0)): 0.2570741641498986, ((1, 2), (-1, 0)): 0.06684285950795832, ((1, 3), (0, 1)): 0.04814863632937083, ((1, 3), (0, -1)): 0.7549754613581506, ((1, 3), (1, 0)): 0.4456632152910486, ((1, 3), (-1, 0)): 0.4032821959580468, ((2, 0), (0, 1)): 0.0021117098253395916, ((2, 0), (0, -1)): 0.018541623109054717, ((2, 0), (1, 0)): 0.768403489884513, ((2, 0), (-1, 0)): 0.614922611557403, ((2, 1), (0, 1)): 0.4617300669243688, ((2, 1), (0, -1)): 0.11396104561360498, ((2, 1), (1, 0)): 0.5367581947048097, ((2, 1), (-1, 0)): 0.1821642476291887, ((2, 2), (0, 1)): 0.12485423005906959, ((2, 2), (0, -1)): 0.19911570253642707, ((2, 2), (1, 0)): 0.7517804312791901, ((2, 2), (-1, 0)): 0.9756595120111766, ((2, 3), (0, 1)): 0.3478225304198034, ((2, 3), (0, -1)): 0.22217909906531086, ((2, 3), (1, 0)): 0.39286358824621703, ((2, 3), (-1, 0)): 0.5790913511734246, ((3, 0), (0, 1)): 0.305945916798136, ((3, 0), (0, -1)): 0.8513062004404177, ((3, 0), (1, 0)): 0.47666896639103706, ((3, 0), (-1, 0)): 0.5164421986926105, ((3, 1), (0, 1)): 0.8769782739423468, ((3, 1), (0, -1)): 0.8738364428103633, ((3, 1), (1, 0)): 0.08607823451927943, ((3, 1), (-1, 0)): 0.274180299890868, ((3, 2), (0, 1)): 0.8502456374539579, ((3, 2), (0, -1)): 0.4775801475787952, ((3, 2), (1, 0)): 0.6088188306881466, ((3, 2), (-1, 0)): 0.7940858514233553, ((3, 3), (0, 1)): 0.5407047350990392, ((3, 3), (0, -1)): 0.1814543673126846, ((3, 3), (1, 0)): 0.8288744964268818, ((3, 3), (-1, 0)): 0.4984471693007364}\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:03<00:00, 2675.93it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["{((0, 0), (0, 1)): 4.466818163620727,\n"," ((0, 0), (0, -1)): 1.027006496327798,\n"," ((0, 0), (1, 0)): -0.6194654928705995,\n"," ((0, 0), (-1, 0)): 0.10435860043112477,\n"," ((0, 1), (0, 1)): -2.2842879651791206,\n"," ((0, 1), (0, -1)): 1.3276435841110203,\n"," ((0, 1), (1, 0)): 5.585481075993956,\n"," ((0, 1), (-1, 0)): -8.59438145520026,\n"," ((0, 2), (0, 1)): 6.690672676122251,\n"," ((0, 2), (0, -1)): -6.735555680317456,\n"," ((0, 2), (1, 0)): 0.42048021258915325,\n"," ((0, 2), (-1, 0)): -4.780906224845939,\n"," ((0, 3), (0, 1)): -1.532805788414846,\n"," ((0, 3), (0, -1)): 1.5319047940761978,\n"," ((0, 3), (1, 0)): 7.802903578577042,\n"," ((0, 3), (-1, 0)): 2.209794867438643,\n"," ((1, 0), (0, 1)): -4.83838277345856,\n"," ((1, 0), (0, -1)): -18.580337646613838,\n"," ((1, 0), (1, 0)): -1.3631571182034856,\n"," ((1, 0), (-1, 0)): 3.4101373752912574,\n"," ((1, 1), (0, 1)): 2.315373497432388,\n"," ((1, 1), (0, -1)): -1.2604222931914084,\n"," ((1, 1), (1, 0)): 6.732552549037726,\n"," ((1, 1), (-1, 0)): 2.2211430795730096,\n"," ((1, 2), (0, 1)): -14.710218378292222,\n"," ((1, 2), (0, -1)): 5.256920617907915,\n"," ((1, 2), (1, 0)): -0.27751653933803094,\n"," ((1, 2), (-1, 0)): 0.280472687578023,\n"," ((1, 3), (0, 1)): 1.1268672039562049,\n"," ((1, 3), (0, -1)): -11.374423939908109,\n"," ((1, 3), (1, 0)): 8.88739835039895,\n"," ((1, 3), (-1, 0)): 1.9305941297370148,\n"," ((2, 0), (0, 1)): 6.644776891849001,\n"," ((2, 0), (0, -1)): 0.8898396154228375,\n"," ((2, 0), (1, 0)): -1.6919253520195328,\n"," ((2, 0), (-1, 0)): -1.8896556468995744,\n"," ((2, 1), (0, 1)): 5.307405372131565,\n"," ((2, 1), (0, -1)): 3.8920955140877718,\n"," ((2, 1), (1, 0)): 7.81063114796608,\n"," ((2, 1), (-1, 0)): 3.9037889197341644,\n"," ((2, 2), (0, 1)): 2.1900579107860625,\n"," ((2, 2), (0, -1)): 1.923967544291711,\n"," ((2, 2), (1, 0)): 8.876370189889471,\n"," ((2, 2), (-1, 0)): -1.0905682301557889,\n"," ((2, 3), (0, 1)): 4.060821014692598,\n"," ((2, 3), (0, -1)): 3.6158835377991436,\n"," ((2, 3), (1, 0)): 9.999999999837726,\n"," ((2, 3), (-1, 0)): 3.4711000767989684,\n"," ((3, 0), (0, 1)): 7.7336077589147525,\n"," ((3, 0), (0, -1)): -2.3503692786289596,\n"," ((3, 0), (1, 0)): -1.5229941031496994,\n"," ((3, 0), (-1, 0)): -1.37880766482922,\n"," ((3, 1), (0, 1)): 8.902031399761215,\n"," ((3, 1), (0, -1)): 5.288931720004431,\n"," ((3, 1), (1, 0)): 6.34658930190492,\n"," ((3, 1), (-1, 0)): 5.396262552730244,\n"," ((3, 2), (0, 1)): 9.999999999999911,\n"," ((3, 2), (0, -1)): 6.592141355327655,\n"," ((3, 2), (1, 0)): 7.6588194299320165,\n"," ((3, 2), (-1, 0)): 6.57400382235146,\n"," ((3, 3), (0, 1)): 0.5407047350990392,\n"," ((3, 3), (0, -1)): 0.1814543673126846,\n"," ((3, 3), (1, 0)): 0.8288744964268818,\n"," ((3, 3), (-1, 0)): 0.4984471693007364}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# start from state (0,0) and take actions based on the learned policy\n","\n","gw.reset()\n","\n","while not gw.terminated:\n","  current_state = gw.state\n","  action = max(gw.action_space, key=lambda a: Q[(current_state, a)])\n","  next_state, reward,_ = gw.step(action)\n","  print(current_state, action, reward)\n","\n","print()\n","print(\"total reward: \", gw.total_reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfedmUT3_31Q","executionInfo":{"status":"ok","timestamp":1712994840193,"user_tz":-330,"elapsed":19,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"06a3e356-6003-49e5-95c9-76f64414b4a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, 0) (0, 1) -1\n","(0, 1) (1, 0) -1\n","(1, 1) (1, 0) -1\n","(2, 1) (1, 0) -1\n","(3, 1) (0, 1) -1\n","(3, 2) (0, 1) 10\n","(3, 3) (1, 0) nan\n","\n","total reward:  5\n"]}]},{"cell_type":"code","source":["class MonteCarloAgant:\n","  def __init__(self,env):\n","    self.env = env\n","    self.reset()\n","\n","  def reset(self):\n","    self.trained = False\n","    gw = self.env\n","\n","    # Initialize state-action values Q(s, a)\n","    self.Q = dict()\n","    for state in gw.observation_space:\n","        for action in gw.action_space:\n","            self.Q[(state, action)] = np.random.random()\n","\n","  def train(self, num_episodes = 10000,epsilon = 0.1):\n","\n","    Q = self.Q.copy()\n","    gw = self.env\n","\n","    # Monte Carlo On-Policy Prediction\n","    for episode in tqdm(range(num_episodes)):\n","        gw.reset()\n","        episode_history = []\n","\n","        # start from a random state to start the episode\n","        gw.state = random.choice(gw.observation_space)\n","\n","        while not gw.terminated:\n","            current_state = gw.state\n","\n","            # Epsilon-greedy policy\n","            if random.random() < epsilon:\n","                action = random.choice(gw.action_space)\n","            else:\n","                action = max(gw.action_space, key=lambda a: Q[(current_state, a)])\n","\n","            next_state, reward, _ = gw.step(action)\n","            episode_history.append((current_state, action, reward))\n","\n","        # Update state-action values based on the returns received in the episode\n","        G = 0\n","        for t in range(len(episode_history) - 2, -1, -1):\n","            state_t, action_t, reward = episode_history[t]\n","            G = reward + 1 * G\n","            # N[(state_t, action_t)] += 1\n","            Q[(state_t, action_t)] += 0.01*(G - Q[(state_t, action_t)])# / N[(state_t, action_t)]\n","\n","    self.Q = Q.copy()\n","    self.trained = True\n","\n","\n","  def policy(self):\n","    gw= self.env\n","    current_state = self.env.state\n","\n","    if self.trained:\n","      action = max(self.env.action_space, key=lambda a: self.Q[(current_state, a)])\n","    else:\n","      action = random.choice(gw.action_space)\n","    return action\n"],"metadata":{"id":"0r4aaYcYdQbi","executionInfo":{"status":"ok","timestamp":1715317953691,"user_tz":-330,"elapsed":7,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["gw = GridWorldEnv(5,5)\n","agent = MonteCarloAgant(gw)\n","\n","gw.reset()\n","is_terminated = gw.terminated\n","\n","agent.train()\n","gw.reset()\n","\n","while not is_terminated:\n","  current_state = gw.state\n","  action = agent.policy()\n","  next_state, reward, is_terminated = gw.step(action)\n","  print(current_state, action, next_state, reward)\n","\n","print(\"\\ntotal reward: \", gw.total_reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxU8Zha6fhrQ","executionInfo":{"status":"ok","timestamp":1712999172384,"user_tz":-330,"elapsed":9368,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"85ff4807-d017-4339-f53f-cbd848a8534b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:09<00:00, 1109.37it/s]"]},{"output_type":"stream","name":"stdout","text":["(0, 0) (1, 0) (1, 0) -1\n","(1, 0) (0, 1) (1, 1) -1\n","(1, 1) (0, 1) (1, 2) -1\n","(1, 2) (1, 0) (2, 2) -1\n","(2, 2) (0, 1) (2, 3) -1\n","(2, 3) (1, 0) (3, 3) 10\n","(3, 3) (1, 0) (3, 3) nan\n","\n","total reward:  5\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Task\n","1. Create a `MonteCarloAgent` class, the `train()` method should perform the training though monte carlo control method.\n","\n","2. Train an agent using the dynamic programming method (use `class DPAgent` and it's `.train()` method to train. ref. Lab 04).\n","3. Evaluate the agent traned in #2 using the Monte Carlo Prediction method and validate whether the predicted state values are equal to the optimal values obtained in #3 though dynamic programming."],"metadata":{"id":"x_iANY34uUkP"}}]}