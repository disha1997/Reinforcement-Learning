{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhLemrQdiuNUf63DBjDrOZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k4G7j4e-oL6F"},"outputs":[],"source":["import itertools\n","import numpy as np\n","import pandas as pd\n","import random"]},{"cell_type":"code","source":["class GridWorldEnv:\n","  def __init__(self, N = 10, M = 10):\n","    # States of a gridworld\n","    self.N = N\n","    self.M = M\n","\n","    # state space\n","    self.observation_space = list(itertools.product(range(self.N), range(self.M)))\n","    # action space\n","    self.action_space = [(0,1), (0,-1), (1,0), (-1,0)]\n","\n","    self.terminal_states = [(3,3)]\n","    self.reset()\n","\n","  def reset(self):\n","    self.state = (0,0)\n","    self.is_terminated = False\n","    self.total_reward = 0\n","\n","  def get_transition_probaility(self, start_state, action, end_state):\n","    if start_state in self.terminal_states:\n","      return 0\n","\n","    expected_state = tuple(np.array(start_state) + np.array(action))\n","    if expected_state == end_state:\n","      return 1\n","\n","    if expected_state not in self.observation_space and start_state == end_state:\n","      return 1\n","\n","    return 0\n","\n","  def get_reward(self, start_state, action, end_state):\n","    if end_state in self.terminal_states:\n","      return 10\n","    else:\n","      return -1\n","\n","  def step(self, action):\n","    if self.state in self.terminal_states:\n","      self.is_terminated = True\n","      reward = np.nan\n","      return self.state, reward, self.is_terminated\n","\n","    current_state = self.state\n","    max_prob = 0\n","    for possible_state in self.observation_space:\n","      p = self.get_transition_probaility(current_state, action, possible_state)\n","      if p > max_prob:\n","        max_prob = p\n","        next_state = possible_state\n","\n","    reward = self.get_reward(current_state, action, next_state)\n","\n","    self.state = next_state\n","    self.total_reward += reward\n","\n","    return self.state, reward, self.is_terminated\n","\n","\n","class RandomActionAgent:\n","  def __init__(self, env):\n","    self.env = env\n","\n","  def policy(self):\n","    action = random.choices(self.env.action_space)[0]\n","    return\n","\n","  def train(self):\n","    pass"],"metadata":{"id":"NDed4dkEoorn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = GridWorldEnv(5,5)"],"metadata":{"id":"kk2wmPXtqp9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hH0RL8n_tqdy","executionInfo":{"status":"ok","timestamp":1709359640929,"user_tz":-330,"elapsed":13,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"7d558e0c-ab35-468e-83f2-87b041487ef0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["action = (1, 0)\n","next_state, reward, is_terminated = env.step(action)\n","\n","print(\"Next state: \", next_state)\n","print(\"Reward: \", reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h97T-E9kxrJ4","executionInfo":{"status":"ok","timestamp":1709359774804,"user_tz":-330,"elapsed":399,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"7914303d-33ab-49dc-c29d-152edef9e1ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Next state:  (1, 0)\n","Reward:  -1\n"]}]},{"cell_type":"markdown","source":["### Random walk"],"metadata":{"id":"9ryeIU82yHut"}},{"cell_type":"code","source":["env.reset()\n","agent = RandomActionAgent(env)\n","\n","while not env.is_terminated:\n","  current_state = env.state\n","  action =  agent.policy()\n","  next_state, reward, is_terminated = env.step(action)\n","  # print(current_state, action, reward, next_state)\n","\n","env.total_reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfI0OeZ-4dkt","executionInfo":{"status":"ok","timestamp":1709361566981,"user_tz":-330,"elapsed":20,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"da027ccb-a0ed-4236-a475-c36ab980534e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-37"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["class DPAgent:\n","  def __init__(self, env):\n","    self.env = env\n","    self.gamma = 1\n","\n","    self.v = dict(zip(self.env.observation_space, np.zeros(self.env.N*self.env.M)))\n","    self.is_trained = False\n","\n","\n","  def policy(self):\n","    if not self.is_trained:\n","      action = random.choices(self.env.action_space)[0]\n","    else:\n","      s = self.env.state\n","      max = -np.inf\n","      for a in self.env.action_space:\n","        term = 0\n","        for s_prime in self.env.observation_space:\n","          term+= self.env.get_transition_probaility(s, a, s_prime) * (self.env.get_reward(s, a, s_prime) + self.gamma * self.v[s_prime])\n","        if term > max:\n","          max = term\n","          action = a\n","\n","    return action\n","\n","  def train(self, iter_limit = 1000):\n","\n","    print(\"performing training...\")\n","\n","    self.v = dict(zip(self.env.observation_space, np.zeros(self.env.N*self.env.M)))\n","\n","    iter = 0\n","    while iter< iter_limit:\n","      for s in self.env.observation_space:\n","        max = -np.inf\n","        for a in self.env.action_space:\n","          term2 = 0\n","          for s_prime in self.env.observation_space:\n","            term2+= self.env.get_transition_probaility(s, a, s_prime) * (self.env.get_reward(s, a, s_prime) + self.gamma*self.v[s_prime])\n","          if term2 > max:\n","            max = term2\n","        self.v[s] = max\n","      iter+=1\n","\n","    self.is_trained = True\n","\n","    print(np.array(list(self.v.values())).reshape(self.env.N, self.env.M))"],"metadata":{"id":"FBoOl5375EQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = GridWorldEnv(5,5)\n","dp_agent = DPAgent(env)\n","\n","dp_agent.train(iter_limit = 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19XWoAYY7NM-","executionInfo":{"status":"ok","timestamp":1709363753573,"user_tz":-330,"elapsed":1490,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"35cb850f-1a78-44dc-a705-b3c84c3a6678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["performing training...\n","[[ 5.  6.  7.  8.  7.]\n"," [ 6.  7.  8.  9.  8.]\n"," [ 7.  8.  9. 10.  9.]\n"," [ 8.  9. 10.  0. 10.]\n"," [ 7.  8.  9. 10.  9.]]\n"]}]},{"cell_type":"code","source":["env = GridWorldEnv(5,5)\n","dp_agent = DPAgent(env)\n","\n","dp_agent.train(iter_limit = 100)\n","\n","env.reset()\n","\n","while not env.is_terminated:\n","  current_state = env.state\n","  action =  dp_agent.policy()\n","  next_state, reward, is_terminated = env.step(action)\n","  # print(current_state, action, reward, next_state)\n","\n","env.total_reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDRdUYivBLZu","executionInfo":{"status":"ok","timestamp":1709363936838,"user_tz":-330,"elapsed":1493,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"efd89f85-ade6-4c04-ff27-a583dc62262f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["performing training...\n","[[ 5.  6.  7.  8.  7.]\n"," [ 6.  7.  8.  9.  8.]\n"," [ 7.  8.  9. 10.  9.]\n"," [ 8.  9. 10.  0. 10.]\n"," [ 7.  8.  9. 10.  9.]]\n"]},{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["Create a heatmap of optimal state values of 10 X 10 gridworld"],"metadata":{"id":"PTsuYtlKC3t5"}},{"cell_type":"code","source":[],"metadata":{"id":"YZN9aO7pDBFK"},"execution_count":null,"outputs":[]}]}