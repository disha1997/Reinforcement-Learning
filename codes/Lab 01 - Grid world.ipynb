{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNud6v41P+7Ji9HmzU9vhIn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":69,"metadata":{"id":"KjAwMCTfSUjs","executionInfo":{"status":"ok","timestamp":1707398634333,"user_tz":-330,"elapsed":613,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"outputs":[],"source":["import itertools\n","import numpy as np\n","import pandas as pd\n","import random"]},{"cell_type":"markdown","source":["  ## Grid world\n","  It has N x M states. The Agent can move from one state to another state depending on the state it is currently in and the action it takes.\n","\n","  Here, there are total of 5*5 = 25 states representing the \"environment\".\n","  \n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |(0,0)|(0,1)|(0,2)|(0,3)|(0,4)|\n","  | 1 |(1,0)|(1,1)|(1,2)|(1,3)|(1,4)|\n","  | 2 |(2,0)|(2,1)|(2,2)|(2,3)|(2,4)|\n","  | 3 |(3,0)|(3,1)|(3,2)|(3,3)|(3,4)|\n","  | 4 |(4,0)|(4,1)|(4,2)|(4,3)|(4,4)|\n","  \n","____________________________\n","### Example\n","**agent is at (2,2)**:\n","\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |[ A ]|     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***right***. Action: (0,1)\n","\n","next_state = current_state + action = (2,2) + (0,1) = (2,3)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |[ A ]|     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***down***. Action: (1,0)\n","\n","next_state = current_state + action = (2,3) + (1,3) = (3,3)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |[ A ]|     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***right***. Action: (0,1)\n","\n","next_state = current_state + action = (3,3) + (0,1) = (3,4)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |[ A ]|\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***down***. Action: (1,0)\n","\n","next_state = current_state + action = (3,4) + (1,0) = (4,4)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     |[ A ]|"],"metadata":{"id":"mutfQDpOWDph"}},{"cell_type":"code","source":["class GridWorldEnv:\n","  \"\"\"Grid world is an one of most classical reinforcement learning problems.\n","  Here the envirnment is made of a rectangular 2-d box with N rows and M columns,\n","  totally consisting of N x M states.\n","\n","  The agent at any given point can take one of the 4 possible actions - to move:\n","  - Left (0,-1)\n","  - Right (0,1)\n","  - Up (-1,0)\n","  - Down (1,0)\n","\n","  Different types of rewards and constraints can be formulated in this kind of\n","  setup\n","\n","  Reward\n","  ------\n","  - Moving to the terminal state receives a reward of +10\n","  - Every other step receives -1 reward.\n","\n","  Teminal states:\n","  (4,4)\n","\n","  |   | 0   | 1   | 2   | 3   | 4   |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |start|     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","  \"\"\"\n","  def __init__(self, N = 10, M = 10):\n","    self.N = N\n","    self.M = M\n","    self.total_states = N*M\n","\n","    self.observation_space = list(itertools.product(range(N), range(M)))\n","    self.action_space = [(0,1), (1,0), (0, -1), (-1, 0)]\n","\n","    self.terminated = False\n","    self.total_reward = 0\n","\n","    # once the agent reaches terminal state, it stays there. Assume start state cannot be terminal state.\n","    # also known as absorbing states\n","    self.terminal_states = [(4,4)]\n","    self.reset()\n","\n","  def reset(self):\n","    self.state = (0,0)\n","\n","  def _get_transition_probability(self, present_state, action, next_state):\n","\n","    if present_state in self.terminal_states:\n","      return 0\n","\n","    # If the expected state matches the action taken, return a probability 1\n","    expected_state = tuple(np.array(present_state) + np.array(action))\n","    if expected_state == next_state:\n","      return 1\n","\n","    # Make sure the agent does not go out of the grid\n","    if (expected_state not in self.observation_space\n","        and present_state == next_state):\n","      return 1\n","\n","    return 0\n","\n","  def _get_reward(self,present_state, action, next_state):\n","    if next_state in self.terminal_states:\n","      reward = 10\n","    else:\n","      reward = -1\n","    return reward\n","\n","  def step(self, action):\n","    # check if terminal state is reached\n","    if self.state in self.terminal_states:\n","      self.terminated = True\n","      reward = np.nan\n","      return self.state, reward, self.terminated\n","\n","    max_prob = -np.inf\n","    for possible_state in self.observation_space:\n","      p = self._get_transition_probability(self.state, action, possible_state)\n","      if p > max_prob:\n","        next_state = possible_state\n","        max_prob = p\n","\n","    reward = self._get_reward(self.state, action, next_state)\n","\n","    self.state = next_state\n","    self.total_reward = self.total_reward + reward\n","\n","    return self.state, reward, self.terminated"],"metadata":{"id":"99Xjz2tSTbyL","executionInfo":{"status":"ok","timestamp":1707398716700,"user_tz":-330,"elapsed":8,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","  def __init__(self,env):\n","    self.env = env\n","    env.reset()\n","\n","  def policy(self):\n","    current_state = self.env.state\n","    action = random.choice([(0,1), (1,0)])\n","    # action = random.choice(env.action_space)\n","    # action = (0, 1)\n","    return action"],"metadata":{"id":"IMBgnuazTpYJ","executionInfo":{"status":"ok","timestamp":1707398717463,"user_tz":-330,"elapsed":15,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["env = GridWorldEnv(5,5)\n","agent = Agent(env)\n","\n","agent.policy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZLSw_TWUK5n","executionInfo":{"status":"ok","timestamp":1707398717464,"user_tz":-330,"elapsed":14,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"039292de-6152-4573-e1b7-d7b00c45cc1a"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 0)"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["agent.policy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkRxBU_1U0fy","executionInfo":{"status":"ok","timestamp":1707398718237,"user_tz":-330,"elapsed":20,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"634f7a43-6793-489b-e6dc-5e3a5a56f24f"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 0)"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["env.state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glohNV6arLbg","executionInfo":{"status":"ok","timestamp":1707398718238,"user_tz":-330,"elapsed":16,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"64897bc3-8dac-4f83-f565-bdb65700dcd1"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["a = agent.policy()\n","a, env.step(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAfeVL7QqdOW","executionInfo":{"status":"ok","timestamp":1707398718239,"user_tz":-330,"elapsed":12,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"427fb7e0-8836-4dea-97e0-a1524b4aa5db"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1, 0), ((1, 0), -1, False))"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"nvg_ZklNfWAe","executionInfo":{"status":"ok","timestamp":1707398718967,"user_tz":-330,"elapsed":13,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["# random walk\n","# get total reward\n","# try with different policies\n","episodes = 10000\n","\n","reward_sum = 0\n","for i in tqdm(range(episodes)):\n","  env = GridWorldEnv(5,5)\n","  agent = Agent(env)\n","  while not env.terminated:\n","    current_state = env.state\n","    action = agent.policy()\n","    next_state, reward, terminated = env.step(action)\n","  reward_sum += env.total_reward\n","\n","  # print(\"current_state:\",current_state, \" action: \", action, \"next_state: \", next_state, \"reward: \", reward)\n","\n","print(\"\\nAverage total reward: \", reward_sum/episodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_bi0j6FUXUI","executionInfo":{"status":"ok","timestamp":1707398734162,"user_tz":-330,"elapsed":15203,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"5e13ad07-f334-4095-f198-c37ea1d3c3ed"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:14<00:00, 674.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Average total reward:  0.781\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["env.total_reward"],"metadata":{"id":"JpfmG2w_YVVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707398734164,"user_tz":-330,"elapsed":47,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"431eea43-91a8-4596-b89c-3ccc3a0764d3"},"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-3"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","source":["### 📔 Tasks\n","- Obtain the average total reward received by taking random actions.\n","- Add dummy states as [(3,0), (2,3)]. Dummy states are states where the agents cannot move to. Hint: modify the transition probability such that the agent cannot move to these states, similar to how we control the agent from moving out of the grid.\n","- Does this change the average total reward received by the agent.\n","\n"],"metadata":{"id":"kBteaFofd_-1"}},{"cell_type":"code","source":[],"metadata":{"id":"-Wdgrl-FxYDy","executionInfo":{"status":"ok","timestamp":1707398734165,"user_tz":-330,"elapsed":39,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":87,"outputs":[]}]}